\documentclass[11pt]{article}

\usepackage[a4paper, margin=1in]{geometry}
\usepackage[british]{babel}
\usepackage[backend=biber, style=numeric-comp]{biblatex}

\addbibresource{bibliography.bib}

\title{Lambda Calculus and its Impact on Computer Science}
\author{Joris Klaasse Bos\\ Zaanlands Lyceum}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

% I still need to decide wether I need a preface and afterword

\section{Preface}

% TODO: Something about PWS

The reason I chose this subject is that it combines two things I enjoy:
abstract maths and computer science. I have been programming for about five to
six years now. I mainly enjoy low-level programming, so, naturally, C is my
most used language and I am most familiar with a simple procedural paradigm.
Such a paradigm is, however, not always very easy to use when working with very
large and complex systems. I, as many, started out with Object-Oriented
Programming, but I did not like that very much. Therefore, I have been
exploring alternative paradigms, including Data-Oriented Programming and
Functional Programming. I am quite familiar with Data-Oriented Programming and
the Rust programming language by now, but Functional Programming isn't
something I've ever really got into yet. I did find out about lambda calculus
and combinatory logic, which intrigued me, but I haven’t got into it beyond a
basic level of understanding. That is why I decided to research it for this
paper. 

\section{Introduction}

With the decline of OOP, many other paradigms are gaining in popularity. One
increasingly popular paradigm is functional programming. Functional programming
is fundamentally based on lambda calculus and it has been seeping into other
paradigms and into mainstream languages. Most of the popular languages now
implement lambda functions and have ways to write in a more declarative style
of programming. In this paper I will look at all the ways lambda calculus has
influenced computer science and how it may do so in the future.

\section{Introduction to lambda calculus}

Lambda calculus is, as its name suggests, a calculus. A calculus is a system of
manipulating symbols, which by themselves don't have any semantic meaning, in a
way that is somehow meaningful. We all know algebra. Algebra itself doesn't
have an innate meaning, but we can use it to represent and solve real world
problems. Algebra, however, is limited. Not every problem can be represented in
algebra. There are many branches of mathematics that use different systems.
One example would be formal logic, which is used for logical operations on
booleans. Another such example is lambda calculus.

\subsection{A short history\label{history}}

People always trusted mathematics to be true and relied on it heavily. If
something was proven true with mathematical logic, then that must be true.
However, starting from the late 19th century, people ran into paradoxes. People
made a distinction between reasoning that was rigorous and reasoning that
wasn't---reasoning that was fundamentally purely logical, and reasoning that
wasn't as purely logical. The fact that mathematical logic, which people looked
at for rigorousness, was infested with paradoxes and self-reference was very
troubling for people at the time.

The concept of mathematics and mathematical logic wasn't well defined, so
people started to think about the formalisation of mathematical logic to try
and solve these issues. People wanted a system that would encapsulate all of
mathematical logic. Preferably this system would be simple, clean and
intuïtive.

Throughout the late 19th and early 20th centuries, people started formally
defining and redefining different aspects of mathematics. \textcite{frege1879}
wrote about propositional calculus and functions as graphs, and in doing so
reëvaluated the concept of functions and was already using concepts like
currying functions without really giving thought to it. \textcite{peano1889}
invented the Peano axioms and Peano arithmetic as a way of defining natural
numbers. He was not the first to attempt defining natural numbers, but he was
the most successful. \textcite{schonfinkel1924} invented combinatory logic as a
way to remove the need for quantified variables, which was later rediscovered
and improved on by \textcite{curry1930}. One major attempt to define all of
mathematics was done by \textcite{russell1997}. They wrote a book that would
become well know in all of mathematics and logic. This book is called
\emph{Principia Mathematica}. They did, however, run into a few problems, which
arose from self-reference. To solve these problems that this paradoxical
self-reference brought with it, they invented an elaborate system, the theory
of types, to circumvent/eliminate it. It was a very carefully crafted bastion
against self-reference ever coming up in their system, which was not very
simple, clean or intuïtive.

People praised \emph{PM} as they thought they had finally done it; they had
formalised all of mathematical logic, they had realised the dream of grounding
all of mathematics in logic. But in Vienna, Gödel was sceptical of this book.
He started seeing some cracks, he felt that there was something wrong aboout
this attempt. Gödel felt that self-reference was a fundamental part of
mathematical logic. Then the crazy bastard went out and actually proved that
there is no consistent system of axioms whose theorems can be listed by an
effective procedure that is capable of proving all truths about arithmetic of
natural numbers\footnote{This is a definition of the first incompleteness
theorem I got from \textcite{wiki:Incompleteness_theorems}}
\parencite{godel1931}, meaning that such a system is either inconsistent or
incomplete\footnote{Incompleteness means that there are things that are true,
but are not provable.}, greatly disturbing many mathematicians and upending
mathematics as they knew it.

During this time, in this environment of the formalisation of mathematical
logic, \textcite{church1932} invented the lambda calculus. Lambda calculus is a
very simple and minimalistic system of substitution. A little while later,
\textcite{turing1936, turing1937correction} invented turing machines. Turing
machines are conceptual mathematical machines that function based on
state---they were state machines. These could perform all kinds of mathematical
and logical computations. He was not the first to invent computers, but he was
the first to work them out as well as he did (and, as you probably know, he
built one with which he cracked the German enigma code).

There was this problem that has a few different names. It is often known as the
\emph{halting problem} or the \emph{Entscheidungsproblem}, which is German for
\emph{decision problem}. The halting problem and decision problem aren't
exactly synonyms, but they come down to the same thing. Basically, it asks
wether it is possible to know via an algorithm wether a computation will
complete execution or result in an infinite loop. In 1936,
\textcite{turing1936, turing1937correction} spent a long time proving that this
isn't possible using his turing machines, but it didn't get published until
early 1937. Also in 1936, \textcite{church1936} proved the same thing using
lambda calculus and happended to publish it before Turing did. When Turing
finally got around to publishing his proof, he found out that he was beaten to
it by Church. He wasn't to pleased. What is interesting, though, is that lambda
calculus and turing machines take two completely different approaches. Turing
machines funtion entirely on state, while lambda calculus is completely
stateless (we'll look at this later). Turing thought this was interesting too,
so he researched lambda calculus and how it relates to his turing machines, and
proved that they are formally equivalent \parencite{turing1937computability}.
Church and Turing gave their names to the Church-Turing thesis, which basically
states that all computable functions can by computed by a Turing machine or
lambda calculus---in other words, lambda calculus is turing complete.

Why do I tell you all this? Well, your main takeaway should be that even though
lambda calculus is a very simple system, it is turing complete. Lambda calculus
and turing machines take wildly different approaches: one state based, the
other stateless. Another difference is that turing machines can be physically
built. We can, however, use lambda calculus on these turing machines and
simulate turing machines with lambda calculus, which is part of the thesis of
this paper. We will look at lambda calculus and how the work of all the
previously mentioned mathematicians, and many more, can be applied in lambda
calculus to get a turing complete system.

\subsection{The syntax}

Lambda calculus is all about first-class higher-order pure
(anonymous\footnote{The core lambda calculus has no way of naming functions.})
unary functions. Such a function takes a single input, and returns a single
expression that is only dependent on the input, so it doesn't have any outside
state. Such a function can take and return any expression, which in lambda
calculus is always a function. A simple function definition in lambda calculus
looks as follows: \[\lambda a.a\]

The lambda signifies a function. Everything following it will be part of that
function's definition. The \(a\) before the \(.\) is the name of the argument.
There is only one, because, as I said before, all functions in lambda calculus
are unary. Everything following the \(.\) is part of the function body, which
is the return expression. The funcion above is the identity function in lambda
calculus; it just returns the input. This is the equivalent of multiplying by
one, or defining a function like \(f(x)=x\), or multiplying a vector with the
identity matrix; it does nothing.

But how do we use this function? Well, just like defining a function, it is
quite simple. If you want to apply this function to a symbol, you just put it
in front of the symbol. Something like this:
\[(\lambda a.a)x\]
Which evaluates to \(x\), because you remove the \(x\) and then replace all the
\(a\)'s in the function body with \(x\) and then remove the function signifier
and argument list. It all comes down to a simple process of substitution.

In this case you need parentheses around the function, otherwise \(x\) would be
considered part of the function's body, which it isn't. It's also important to
note that lambda calculus is left-associative, that is, it evaluates an
expression from left to right. This means that the function on the far left of
an expression gets invoked first.

I have now basically explained the entire lambda calculus, it is really that
simple. I have explained abstraction (functions), application (applying
functions), and grouping (parentheses), which is basically all we need. You
can also give names to expressions. We could name our identity function \(I\)
as follows: \[I:=\lambda a.a\] But this isn't really part of the core lambda
calculus anymore, just some syntactic sugar. This way, instead of constantly
having to write \(\lambda a.a\), we can just write \(I\). So instead of
writing: \[(\lambda a.a)x=x\] We could use our previous definition of \(I\) and
write: \[Ix=x\] We have now covered identifiers too.

But if this is all there is, how can this possibly be Turing complete? How do
we do boolean logic, or algebra? How can we do things with only unary
functions? What are \(a\) and \(x\) supposed to represent? If there is no
concept of value, how do we even use this meaningfully?

Well, the key is this: a function can return any expression (remember?), which
is always a function\footnote{Everything is.}, not just a single symbol. We can
start composing these simple functions into more complex functions. Let's say
that we wanted to have a function that takes two arguments, and then applies
the first argument to the second one. You are probably asking yourself a few
questions. For example, what does it mean for one argument to be applied to
another? Well, as I said, everything is a function. But the biggest question
you are probably asking yourself is: how can you have a function that takes two
arguments?

We actually can't, but what we can do is to have a function that takes one
argument and returns another function that takes one argument. We can define
that function as follows:
\[\lambda a.\lambda b.ab\]
We currently have a function definition inside the body of another function. If
we now apply this function to a symbol like \(x\), we get this:
\[(\lambda a.\lambda b.ab)x=\lambda b.xb\]
We get a new function that takes an argument and applies \(x\) to it. If we now
apply this function to a symbol like \(y\), we get this:
\[(\lambda b.xb)y=xy\]
Alternatively, we could write it all on one line:
\[(\lambda a.\lambda b.ab)xy=(\lambda b.xb)y=xy\]

\(xy\) in this case is what we would call the \emph{\(\beta\)-normal form} of
the preceding expressions. That just means that it is in the simpelest form and
isn't able to be evaluated any further. Reducing a lambda expression to the
\(\beta\)-normal form is called \emph{\(\beta\)-reduction}.

You can start to see how we can compose unary functions to create more complex
functions\footnote{That's what makes them higher-order (and first-class).}. In
this example we used two nested unary functions to get the same result you
would with a binary function. Such a nested function is often called a
\emph{curry'd function}.

You might think to yourself that having this many nested functions can be quite
convoluted and not very readable, and you're quite right. That's why people
often use a shorthand notation. They would basically write it as if it is a
single binary function (as with any n-ary function). They would write the
example function above as:
\[\lambda ab.ab\]
Do keep in mind, that even though this looks and, for the most part, acts as if
it is a single binary function, it really isn't. It still is a curry'd function
that feeds in the arguments one-by-one, but this way the expression becomes
more readable and easier to think about conceptually. We will use this notation
from now on.

Congratulations, you now know the very basics of lambda calculus. You may still
not see how this is Turing complete or how this can be useful and meaningful.
You might also already see some of the intrigues of lambda calculus; how simple
it is, how it doesn't have a concept of value or data, how everything is an
expression, how it is stateless, etc. But we'll get to all of that eventually.
If you get this, everything else will follow naturally (mostly).

\subsection{Combinatory logic}

Combinatory logic is a notation to eliminate the need for quantified variables
in mathematical logic \parencite{wiki:Combinatory_logic}. That basically means
a form of logic without values, just like with lambda calculus, but just pure
logical expressions, using so called \emph{combinators}. The idea of
combinators first came from \textcite{schonfinkel1924}, and was later
rediscovered by \textcite{curry1930}. Combinators are just symbols, in this
case letters, that perform operations on symbols that succeed it. We've
actually looked at one of these combinators already.

\subsubsection{Identity}

The first combinator we'll cover is \(I\). It does exactly the same thing as
our \(I\) function we defined in lambda calculus in the previous subsection
(\(I:=\lambda a.a\)). In fact, all combinators can be defined in lambda
calculus. Lambda calculus is really just 90\% combinatory logic, but without
identifiers. This combinator may seem quite useless, but it is actually quite
useful when composing combinators, which we'll come to soon.

\subsubsection{The omega combinator}

The next combinator we'll cover is \(M\). All it does is repeat its one
argument twice. It can be defined in lambda calculus as follows:
\[M:=\lambda f.ff\]

We could, for example, look at what happens when you apply \(M\) to \(I\). We
get:
\[M I = I I = I\]
Or, written out in lambda calculus:
\[(\lambda f.ff)\lambda a.a=(\lambda a.a)\lambda a.a=\lambda a.a\]

What happens if you apply \(M\) to \(M\)? You get:
\[M M = M M = M M = ...\]
and so on to infinity. Or in lambda calculus:
\[(\lambda f.ff)\lambda f.ff=(\lambda f.ff)\lambda f.ff=(\lambda f.ff)\lambda f.ff=...\]

This expression cannot be evaluated. We say that it doesn't have a
\(\beta\)-normal form. In lambda calculus and combinatory logic not every
expression is reducable. As we've seen in the second to last paragraph of
section \ref{history}: there is no single algorithm to decide wether a lambda
expression has a \(\beta\)-normal form.

\(M M\) is sometimes called the \(\Omega\) combinator. Omega, because it is the
end of the Greek alphabet. The \(M\) combinator is sometimes called the
\(\omega\) combinator because of this. Combinators often have many different
names. Sometimes because scientists discovered them seperatly, unaware of
eachother, sometimes because they preferred a different name, sometimes because
scientists like to give them pet names \footnote{I have a theory they are just
trying to throw us off}.

\subsubsection{The constant combinator}

The next combinator we'll cover is \(K\). It is a combinator that takes two
arguments and returns the first. We can easily define it in lambda calculus as
follows:
\[K:=\lambda ab.a\]

Remember that we defined this as a curry'd function. This means we can give it
just one argument and get a new function out of it. Let's say we apply \(K\) to
\(5\):
\[K5=(\lambda ab.a)5=\lambda b.5\]
Our new function, \(K5\), is a function that takes an argument and returns
\(5\). This means that whatever we apply this function to, we always get \(5\).
\(K\) gets its name from the German word \emph{Konstant}, meaning constant. You
can probably see why.

Just like with the previous combinators, it'll prove very useful, much more so
than you'd expect.

\subsubsection{The kite}

Here is where things get a little spicier. Our next combinator is \(KI\). It
takes two arguments and returns the latter. We can define it in lambda calculus
as follows:
\[KI:=\lambda ab.b\]

You may already be thinking about its name. Why does it have two letters? And
why are they two letters we've talked about already? Well, the answer is very
simple. If you apply \(K\) to \(I\), you get \(KI\). Don't believe me? Let's
try!

If we use our definition of \(KI\) and apply it to \(xy\) we get:
\[KIxy=(\lambda ab.b)xy=y\]
But if we use the K and I combinators seperately, we get the following:
\[KIxy=(\lambda ab.a)Ixy=(\lambda b.I)xy=Iy=y\]

If you think about it, it is very logical. If \(K\) takes two arguments and
returns the first, then, in this case, it uses up both \(I\) and \(x\) and
returns \(I\), which will just return the next argument, in this case \(y\).
\(KI\) will always return the second symbol after the I, because---again---the
first gets used up by \(K\).

We can also just see what function we get when we apply \(K\) to \(I\):
\[KI=(\lambda ab.a)I=\lambda b.I=\lambda b.\lambda a.a=\lambda ba.a\] We get
our definition of \(KI\) (except the names of the arguments are switched).

We're starting to define combinators as combinations of other combinators.
Every combinator, in fact, can be defined as a combination of other
combinators. That's why they are called combinators.

\subsubsection{The starling}

\subsubsection{The cardinal}

\subsubsection{The bluebird}

\subsubsection{\(S\) and \(K\)}

\subsubsection{To Mock a Mockingbird}

\subsubsection{Equality}

\section{Using lambda calculus for computation}

\subsection{Binary logic}

\subsection{Church numerals}

\section{Functional programming (lambda calculus applied)}

\subsection{Laziness}

% Note to self: write about how lambda functions can create new lambda
% functions, as we've seen in the combinatory logic subsection

\subsection{Haskell}

\section{Functional programming in other paradigms}

\section{Possibilities for the future}

\section{Afterword}

\newpage
\printbibliography[heading=bibintoc, title={References}]
\end{document}
